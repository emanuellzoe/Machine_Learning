# EDA: Dataset Kartu Pokémon Asli vs. Palsu
# Notebook ini bertujuan untuk melakukan Analisis Data Eksplorasi (EDA) pada dataset gambar kartu Pokémon untuk memahami karakteristik yang membedakan kartu asli (real) dan palsu (fake).
# Link : https://www.kaggle.com/datasets/ongshujian/real-and-fake-pokemon-cards/data

## 1. Data Acquisition

### 1.1 Penjelasan Library yang Digunakan
# - **os & glob**: Untuk berinteraksi dengan sistem file, yaitu membaca struktur folder dan menemukan path semua file gambar.
# - **pandas**: Untuk membuat DataFrame yang berisi path file dan labelnya (real/fake).
# - **matplotlib & seaborn**: Untuk visualisasi data, seperti menampilkan contoh gambar dan plot distribusi.
# - **PIL (Pillow)**: Pustaka untuk membuka dan memanulasi file gambar.

# Library untuk sistem file
import os
import glob

# Library untuk manipulasi data dan visualisasi
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Library untuk memproses gambar
from PIL import Image

%matplotlib inline
print("Library siap digunakan!")

### 1.2 Memuat Path Gambar dan Label
# Kita hanya akan fokus pada data training untuk EDA, sesuai dengan praktik terbaik dalam machine learning. Data `test` akan kita simpan untuk evaluasi model nanti. Pastikan Anda telah mengekstrak file zip dari Kaggle dan menempatkan folder `pokemon-cards-dataset` di dalam folder `data`.

# Tentukan path utama ke folder data training dan file label
train_img_path = 'data/train/'
train_labels_path = 'data/train_labels.csv'

# 1. Muat file CSV yang berisi label
labels_df = pd.read_csv(train_labels_path)

# 2. Ubah format kolom 'id' agar cocok dengan nama file (tanpa 'zfill')
labels_df['id'] = labels_df['id'].astype(str)

# 3. Buat kolom 'filepath' yang berisi path lengkap ke setiap gambar
labels_df['filepath'] = labels_df['id'].apply(lambda x: os.path.join(train_img_path, x + '.jpg'))

# 4. Ganti label numerik (0, 1) menjadi teks ('real', 'fake')
label_map = {0: 'real', 1: 'fake'}
labels_df['label'] = labels_df['label'].map(label_map)

# 5. Buat DataFrame akhir yang akan kita gunakan untuk analisis
df = labels_df[['filepath', 'label']].copy()

# Acak urutan data dalam DataFrame
df = df.sample(frac=1, random_state=42).reset_index(drop=True)

print("DataFrame berhasil dibuat ulang dengan path yang benar.")
df.head()

## 2. Data Profiling / Exploratory Data Analysis (EDA)

### 2.1 Analisis Distribusi Kelas (Asli vs. Palsu)
# Melihat perbandingan jumlah gambar antara kartu asli dan palsu.

print("Jumlah gambar per label:")
print(df['label'].value_counts())

# Visualisasi distribusi kelas
plt.figure(figsize=(8, 6))
sns.countplot(x='label', data=df, palette='pastel')
plt.title('Distribusi Kartu Asli vs. Palsu', fontsize=16)
plt.xlabel('Label', fontsize=12)
plt.ylabel('Jumlah Gambar', fontsize=12)
plt.show()

# **Interpretasi**: Dataset ini memiliki sedikit ketidakseimbangan, di mana jumlah kartu palsu (fake) lebih banyak daripada kartu asli (real). Ini perlu diperhatikan saat evaluasi model nanti.

### 2.2 Visualisasi Contoh Gambar
# Melihat perbedaan visual antara kartu asli dan palsu.

# Fungsi untuk menampilkan contoh gambar
def tampilkan_contoh_gambar(label_name, jumlah=5):
    plt.figure(figsize=(20, 6))
    
    contoh_files = df[df['label'] == label_name].sample(jumlah, random_state=1)['filepath'].values
    
    plt.suptitle(f'Contoh Kartu Pokémon {label_name.upper()}', fontsize=20)
    
    for i, file_path in enumerate(contoh_files):
        ax = plt.subplot(1, jumlah, i + 1)
        try:
            img = Image.open(file_path)
            plt.imshow(img)
        except FileNotFoundError:
            print(f"File tidak ditemukan di: {file_path}")
            ax.text(0.5, 0.5, 'File Not Found', ha='center', va='center')
        
        plt.axis('off')
    
    plt.show()

# Tampilkan contoh untuk kartu ASLI
tampilkan_contoh_gambar('real')

# Tampilkan contoh untuk kartu PALSU
tampilkan_contoh_gambar('fake')

# **Interpretasi**: Secara visual, kita mungkin bisa melihat perbedaan dalam saturasi warna, kualitas cetak, atau jenis font antara kartu asli dan palsu.

### 2.3 Analisis Dimensi (Ukuran) Gambar
# Memeriksa apakah semua gambar di dataset kita memiliki ukuran yang sama, karena ini penting untuk input model.

# Fungsi untuk mendapatkan dimensi (lebar, tinggi) dari sebuah gambar
def get_dimensi(file_path):
    try:
        with Image.open(file_path) as img:
            return img.size # Mengembalikan tuple (lebar, tinggi)
    except:
        return (0, 0)

# Terapkan fungsi ke setiap baris dan buat kolom baru
df['dimensions'] = df['filepath'].apply(get_dimensi)

print("Ukuran Gambar yang Paling Umum:")
print(df['dimensions'].value_counts())

# **Interpretasi**: Hasil di atas menunjukkan bahwa sebagian besar gambar memiliki ukuran (734, 1024). Namun, ada juga ukuran lain. Ini mengkonfirmasi bahwa kita perlu melakukan resizing (penyeragaman ukuran) pada tahap preprocessing.

# ==============================================================================
# Bagian 3: Data Preparation / Preprocessing
# ==============================================================================

# Pada tahap ini, kita akan mempersiapkan data gambar agar siap digunakan oleh model machine learning, khususnya model Jaringan Saraf Konvolusional (CNN).
# Proses ini mencakup:
# 1.  **Resizing**: Menyeragamkan ukuran semua gambar ke dimensi yang sama (misalnya, 128x128 piksel). Ini wajib karena input model neural network harus memiliki ukuran yang konsisten.
# 2.  **Normalization**: Mengubah nilai piksel gambar dari rentang [0, 255] menjadi rentang [0, 1]. Ini membantu model belajar lebih cepat dan stabil.
# 3.  **Label Encoding**: Mengubah label teks ('real', 'fake') menjadi angka (0 dan 1) yang bisa dipahami model.
# 4.  **Train-Test Split**: Memisahkan data menjadi set data latih dan data uji untuk melatih dan mengevaluasi model.

# Import library tambahan yang dibutuhkan
from tqdm import tqdm # Untuk menampilkan progress bar
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Tentukan ukuran gambar yang seragam
IMG_SIZE = 128

# Siapkan list untuk menampung data gambar (X) dan labelnya (y)
X = []
y = []

# Loop melalui setiap baris di DataFrame dengan progress bar
for index, row in tqdm(df.iterrows(), total=df.shape[0], desc="Processing Images"):
    try:
        # Buka gambar dan pastikan formatnya RGB
        img = Image.open(row['filepath']).convert('RGB')
        # Ubah ukuran gambar
        img = img.resize((IMG_SIZE, IMG_SIZE))
        # Konversi gambar menjadi numpy array dan normalisasi
        img_array = np.array(img) / 255.0
        
        # Tambahkan array gambar dan label ke list
        X.append(img_array)
        y.append(row['label'])
    except Exception as e:
        print(f"Error memproses file {row['filepath']}: {e}")

# Ubah list menjadi numpy array
X = np.array(X)
y = np.array(y)

print(f"\nPreprocessing selesai. Bentuk data gambar (X): {X.shape}")

# Mengubah label 'real' dan 'fake' menjadi 0 dan 1
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Melihat mappingnya (misal: fake=0, real=1)
print("\nMapping Label:")
for i, label in enumerate(le.classes_):
    print(f"{label} -> {i}")

# Membagi data menjadi 80% data latih dan 20% data uji
# 'stratify=y_encoded' penting untuk memastikan proporsi label 'real' dan 'fake' sama di data latih dan uji
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

print(f"\nBentuk X_train: {X_train.shape}")
print(f"Bentuk X_test: {X_test.shape}")
print(f"Bentuk y_train: {y_train.shape}")
print(f"Bentuk y_test: {y_test.shape}")


## Kesimpulan & Langkah Selanjutnya

# Dari seluruh proses ini, kita mendapatkan beberapa poin penting:
# 1.  **Struktur Data**: Kita berhasil memetakan dataset gambar ke dalam DataFrame yang terstruktur.
# 2.  **Distribusi Kelas**: Kita memahami sebaran jumlah gambar untuk setiap tipe Pokémon.
# 3.  **Karakteristik Gambar**: Kita tahu bahwa ukuran gambar tidak seragam, sehingga perlu di-resize.
# 4.  **Data Siap**: Data gambar telah diubah menjadi format numerik (numpy array) yang sudah di-resize, di-normalisasi, dan dibagi menjadi data latih & uji.

# **Langkah Selanjutnya:**
# Data `X_train`, `X_test`, `y_train`, dan `y_test` sekarang siap untuk tahap **Modeling**, yaitu untuk melatih model *Convolutional Neural Network* (CNN) guna melakukan klasifikasi biner (real vs fake).